{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c34cf307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.0\n",
      "Num GPUs Available:  1\n",
      "Physical GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU 显存设置为按需分配 (Memory Growth Enable)\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from tensorflow import math as TFmath\n",
    "import math,os,shutil\n",
    "from prettytable import PrettyTable\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# 检查 GPU 是否可用\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(gpus))\n",
    "print(\"Physical GPUs:\", gpus)\n",
    "\n",
    "if len(gpus) == 0:\n",
    "    print(\"\\n[警告] 未检测到 GPU。可能有以下原因：\")\n",
    "    print(\"1. 未安装 NVIDIA 显卡驱动或驱动版本过低。\")\n",
    "    print(\"2. 未安装 CUDA Toolkit (需 11.2) 或 cuDNN (需 8.1) - 针对 TF 2.10 Windows版。\")\n",
    "    print(\"3. 如果安装了 TF > 2.10，Windows Native GPU 支持已被移除，需使用 WSL2。\")\n",
    "else:\n",
    "    # 显存按需分配，防止在这步占满显存导致后续报错\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU 显存设置为按需分配 (Memory Growth Enable)\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "tf.random.set_seed(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "988aa91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full paths to training data\n",
    "TRAIN_FEATURE_PATH = \"../Features/Train/train_features.csv\"\n",
    "TRAIN_LABEL_PATH   = \"../Features/Train/train_labels.csv\"\n",
    "\n",
    "# Full path to test data\n",
    "TEST_FEATURE_PATH = \"../Features/Test/test_features.csv\"\n",
    "\n",
    "# Output folders\n",
    "figure_prepath = \"../figures/\"\n",
    "model_prepath = \"../model/sev_se_block_fe/\"\n",
    "\n",
    "# Create output folders if not existed\n",
    "os.makedirs(figure_prepath, exist_ok=True)\n",
    "os.makedirs(model_prepath, exist_ok=True)\n",
    "\n",
    "# Plotting configuration\n",
    "figureFrontSize = 12\n",
    "figureName_post = \"_test.png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "195e85b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### helper labels - fixed\n",
    "Numchannels = 95\n",
    "num_inputFeatures = Numchannels * 2 + 4 # target_gain, target_gain_tilt, EDFA_input_power_total, EDFA_output_power_total\n",
    "labels = {\"gainValue\":'target_gain',\n",
    "          \"EDFA_input\":'EDFA_input_power_total',\n",
    "          \"EDFA_output\":'EDFA_output_power_total',\n",
    "          \"inSpectra\":'EDFA_input_spectra_',\n",
    "          \"WSS\":'DUT_WSS_activated_channel_index_',\n",
    "          \"result\":'calculated_gain_spectra_'}\n",
    "inSpectra_labels = [labels['inSpectra']+str(i).zfill(2) for i in range(0,Numchannels)]\n",
    "onehot_labels = [labels['WSS']+str(i).zfill(2) for i in range(0,Numchannels)]\n",
    "result_labels = [labels['result']+str(i).zfill(2) for i in range(0,Numchannels)]\n",
    "preProcess_labels = [labels['EDFA_input'],labels['EDFA_output']]\n",
    "preProcess_labels.extend(inSpectra_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb88d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dB_to_linear(data):\n",
    "  return np.power(10,data/10)\n",
    "\n",
    "def linear_TO_Db(data):\n",
    "  result = 10*np.log10(data).to_numpy()\n",
    "  return result[result != -np.inf]\n",
    "\n",
    "def linear_TO_Db_full(data):\n",
    "  result = 10*np.log10(data).to_numpy()\n",
    "  result[result == -np.inf] = 0\n",
    "  return result\n",
    "\n",
    "def divideZero(numerator,denominator):\n",
    "  with np.errstate(divide='ignore'):\n",
    "    result = numerator / denominator\n",
    "    result[denominator == 0] = 0\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "018f38b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === 新增：针对 SHB 和全局统计的特征工程函数 ===\n",
    "# def add_shb_features(df):\n",
    "#     df_eng = df.copy()\n",
    "    \n",
    "#     # 1. 提取光谱列 (Input Spectra columns) 和 WSS 状态列\n",
    "#     # 模糊匹配列名\n",
    "#     spectra_cols = [c for c in df.columns if 'EDFA_input_spectra' in c]\n",
    "#     wss_cols = [c for c in df.columns if 'DUT_WSS_activated_channel_index' in c]\n",
    "    \n",
    "#     # 确保按顺序排列\n",
    "#     spectra_cols.sort()\n",
    "#     wss_cols.sort()\n",
    "    \n",
    "#     # 2. SHB 关键统计特征 (Global Statistics)\n",
    "#     # 标准差(std): 直接反映光谱的平坦程度。std 越大，烧孔风险越高。\n",
    "#     df_eng['shb_std']  = df_eng[spectra_cols].std(axis=1)\n",
    "#     # 均值(mean)与总和(sum): 反映总输入功率水平\n",
    "#     df_eng['shb_mean'] = df_eng[spectra_cols].mean(axis=1)\n",
    "# #     df_eng['shb_sum']  = df_eng[spectra_cols].sum(axis=1) \n",
    "# #     # 最大值(max): 局部强光点是 SHB 的主要诱因\n",
    "# #     df_eng['shb_max']  = df_eng[spectra_cols].max(axis=1) \n",
    "    \n",
    "# #     # 3. 活跃信道数 (Load)\n",
    "# #     # 信道越少，单信道分到的功率可能越高，容易产生非线性效应\n",
    "# #     df_eng['feat_active_channel_count'] = df_eng[wss_cols].sum(axis=1)\n",
    "    \n",
    "# #     # 4. 光谱重心 (Spectral Center of Mass)\n",
    "# #     # 衡量输入光功率是集中在长波(红移)还是短波(蓝移)，影响增益倾斜(Tilt)\n",
    "# #     indices = np.arange(len(spectra_cols))\n",
    "# #     # 矩阵乘法快速计算加权和\n",
    "# #     weighted_sum = df_eng[spectra_cols].dot(indices)\n",
    "# #     # 防止除零\n",
    "# #     total_p = df_eng[spectra_cols].sum(axis=1) + 1e-9\n",
    "# #     df_eng['shb_center'] = weighted_sum / total_p\n",
    "    \n",
    "# #     return df_eng\n",
    "# def add_shb_features(df):\n",
    "#     df_eng = df.copy()\n",
    "    \n",
    "#     # 1. 提取光谱列 (Input Spectra columns) 和 WSS 状态列\n",
    "#     spectra_cols = [c for c in df.columns if 'EDFA_input_spectra' in c]\n",
    "#     wss_cols = [c for c in df.columns if 'DUT_WSS_activated_channel_index' in c]\n",
    "#     spectra_cols.sort()\n",
    "#     wss_cols.sort()\n",
    "    \n",
    "#     # === SHB 关键统计特征 ===\n",
    "#     df_eng['shb_std']  = df_eng[spectra_cols].std(axis=1)\n",
    "#     df_eng['shb_mean'] = df_eng[spectra_cols].mean(axis=1)\n",
    "#     df_eng['shb_sum']  = df_eng[spectra_cols].sum(axis=1)\n",
    "#     df_eng['shb_max']  = df_eng[spectra_cols].max(axis=1)\n",
    "    \n",
    "#     # === 活跃信道数 ===\n",
    "#     df_eng['feat_active_channel_count'] = df_eng[wss_cols].sum(axis=1)\n",
    "    \n",
    "#     # === 光谱重心 ===\n",
    "#     indices = np.arange(len(spectra_cols))\n",
    "#     weighted_sum = df_eng[spectra_cols].dot(indices)\n",
    "#     total_p = df_eng[spectra_cols].sum(axis=1) + 1e-9\n",
    "#     df_eng['shb_center'] = weighted_sum / total_p\n",
    "    \n",
    "#     # === 物理特征 ===\n",
    "#     # 线性域总功率 (mW)\n",
    "#     mw_df = np.power(10, df_eng[spectra_cols] / 10.0)\n",
    "#     df_eng['total_power_mW'] = mw_df.sum(axis=1)\n",
    "#     # 动态范围 (Dynamic Range)\n",
    "#     df_eng['power_span'] = df_eng[spectra_cols].max(axis=1) - df_eng[spectra_cols].min(axis=1)\n",
    "    \n",
    "#     return df_eng\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis\n",
    "def add_shb_features(df):\n",
    "    \"\"\"\n",
    "    全能版特征工程函数：集成噪声截断、时域统计、频域FFT、梯度特征、线性域特征\n",
    "    \"\"\"\n",
    "    # 1. 拷贝原始数据\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    # 2. 自动提取列名\n",
    "    spectra_cols = [c for c in df.columns if 'EDFA_input_spectra' in c]\n",
    "    wss_cols = [c for c in df.columns if 'DUT_WSS_activated_channel_index' in c]\n",
    "    spectra_cols.sort()\n",
    "    wss_cols.sort()\n",
    "    \n",
    "    # === [关键策略 1] 噪声底截断 (Noise Floor Clipping) ===\n",
    "    # 必须最先执行！清洗掉 -60dBm 以下的随机噪声，防止它们污染后续的均值/方差计算\n",
    "    # 阈值建议 -55.0，底噪设为 -60.0\n",
    "    print(\"  -> Performing Noise Clipping (< -55dBm to -60dBm)...\")\n",
    "    spectra_matrix = df_eng[spectra_cols].values\n",
    "    spectra_matrix = np.where(spectra_matrix < -55.0, -60.0, spectra_matrix)\n",
    "    # 将清洗后的数据写回 DataFrame (让模型直接看到干净的光谱)\n",
    "    df_eng[spectra_cols] = spectra_matrix \n",
    "    \n",
    "    # === 3. 使用字典收集所有新特征 (解决 PerformanceWarning 碎片化问题) ===\n",
    "    new_feats = {}\n",
    "    \n",
    "    # --- A. 基础统计特征 (Based on Clipped Data) ---\n",
    "    # 使用 numpy 向量化计算，比 pandas apply 快得多\n",
    "    new_feats['shb_std']  = np.std(spectra_matrix, axis=1)\n",
    "    new_feats['shb_mean'] = np.mean(spectra_matrix, axis=1)\n",
    "    new_feats['shb_sum']  = np.sum(spectra_matrix, axis=1)\n",
    "    new_feats['shb_max']  = np.max(spectra_matrix, axis=1)\n",
    "    \n",
    "    # --- B. 差分特征 (Gradient/Shape Features) ---\n",
    "    # 捕捉光谱的局部斜率和曲率，对 Bi-LSTM/CNN 极有帮助\n",
    "    # 一阶差分 (Slope)\n",
    "    grad1 = np.diff(spectra_matrix, axis=1)\n",
    "    new_feats['grad1_mean_abs'] = np.mean(np.abs(grad1), axis=1) # 平均抖动\n",
    "    new_feats['grad1_max'] = np.max(np.abs(grad1), axis=1)       # 最大突变\n",
    "    new_feats['grad1_std'] = np.std(grad1, axis=1)               # 形状复杂度\n",
    "    \n",
    "    # 二阶差分 (Curvature)\n",
    "    grad2 = np.diff(grad1, axis=1)\n",
    "    new_feats['grad2_mean_abs'] = np.mean(np.abs(grad2), axis=1) # 平均弯曲度\n",
    "    \n",
    "    # --- C. WSS 活跃信道 ---\n",
    "    # 这里直接用原始 df 的列即可\n",
    "    new_feats['feat_active_channel_count'] = df_eng[wss_cols].sum(axis=1)\n",
    "    \n",
    "    # --- D. 光谱重心 (Spectral Centroid) ---\n",
    "    indices = np.arange(len(spectra_cols))\n",
    "    # 使用点积加速加权求和\n",
    "    weighted_sum = np.dot(spectra_matrix, indices) \n",
    "    total_p = np.sum(spectra_matrix, axis=1) + 1e-9\n",
    "    new_feats['shb_center'] = weighted_sum / total_p\n",
    "    \n",
    "    # --- E. 物理特征 (线性域 Linear Domain) ---\n",
    "    # 将 dBm 转为 mW，更能反映真实的物理能量叠加\n",
    "    mw_matrix = np.power(10, spectra_matrix / 10.0)\n",
    "    \n",
    "    new_feats['total_power_mW'] = np.sum(mw_matrix, axis=1)\n",
    "    # 偏度 (Skew): 能量偏左还是偏右 (SRS效应指示器)\n",
    "    new_feats['power_skew'] = skew(mw_matrix, axis=1)\n",
    "    # 峰度 (Kurtosis): 光谱是尖锐的还是平坦的\n",
    "    new_feats['power_kurt'] = kurtosis(mw_matrix, axis=1)\n",
    "    # 动态范围\n",
    "    new_feats['power_span'] = np.max(spectra_matrix, axis=1) - np.min(spectra_matrix, axis=1)\n",
    "    \n",
    "    # --- F. FFT 频域特征 (Frequency Domain) ---\n",
    "    # 捕捉周期性纹波 (Ripple)\n",
    "    fft_vals = np.fft.fft(spectra_matrix, axis=1)\n",
    "    fft_abs = np.abs(fft_vals)\n",
    "    \n",
    "    new_feats['fft_mean'] = np.mean(fft_abs, axis=1)\n",
    "    new_feats['fft_std'] = np.std(fft_abs, axis=1)\n",
    "    new_feats['fft_max'] = np.max(fft_abs, axis=1)\n",
    "    \n",
    "    # 提取 FFT 低频分量 (前5个，代表整体轮廓)\n",
    "    for i in range(1, 6): # 跳过直流分量 0\n",
    "        new_feats[f'fft_component_{i}'] = fft_abs[:, i]\n",
    "        \n",
    "    # === 4. 一次性合并 ===\n",
    "    # 将字典转换为 DataFrame，并与原数据拼接\n",
    "    # 这是避免 \"DataFrame is highly fragmented\" 警告的最佳实践\n",
    "    new_feats_df = pd.DataFrame(new_feats, index=df_eng.index)\n",
    "    \n",
    "    df_final = pd.concat([df_eng, new_feats_df], axis=1)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ff6b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "### train and loss function\n",
    "def custom_loss(y_actual,y_pred):\n",
    "  # calculate the loaded channel numbers for each batch\n",
    "  # batch default is [batch size=32, outputchannel number]\n",
    "  loaded_size = tf.dtypes.cast(TFmath.count_nonzero(y_actual), tf.float32)\n",
    "  # turn unloaded y_pred prediction to zero\n",
    "  y_pred_cast_unloaded_to_zero = TFmath.divide_no_nan(TFmath.multiply(y_pred,y_actual),y_actual)\n",
    "  # error [unloaded,unloaded,loaded,loaded]: y_pred = [13->0,15->0,18.5,18.3], y_actual = [0,0,18.2,18.2]\n",
    "  error = TFmath.abs(TFmath.subtract(y_pred_cast_unloaded_to_zero,y_actual))\n",
    "  # custom_loss = (0.3+0.2) / 2\n",
    "  custom_loss = TFmath.divide(TFmath.reduce_sum(error),loaded_size)\n",
    "  return custom_loss\n",
    "\n",
    "def custom_loss_L2(y_actual,y_pred):\n",
    "  loaded_size = tf.dtypes.cast(TFmath.count_nonzero(y_actual), tf.float32)\n",
    "  y_pred_cast_unloaded_to_zero = TFmath.divide_no_nan(TFmath.multiply(y_pred,y_actual),y_actual)\n",
    "  error = TFmath.square(TFmath.subtract(y_pred_cast_unloaded_to_zero,y_actual))\n",
    "  custom_loss = TFmath.sqrt(TFmath.divide(TFmath.reduce_sum(error),loaded_size))\n",
    "  return custom_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7717ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_block(input_tensor, ratio=16):\n",
    "    \"\"\"\n",
    "    Squeeze and Excitation Block for Dense Layers (Feature Reweighting).\n",
    "    Explicitly models interdependencies between channels (features).\n",
    "    \"\"\"\n",
    "    # Input shape: (Batch, input_dim)\n",
    "    input_dim = input_tensor.shape[-1]\n",
    "    \n",
    "    # Squeeze: Global Information Aggregation \n",
    "    # For Dense layers, the input itself is the global descriptor. \n",
    "    # So we proceed directly to Excitation.\n",
    "    \n",
    "    # Excitation: Adaptive Recalibration\n",
    "    # 1. Bottleneck\n",
    "    x = layers.Dense(input_dim // ratio, use_bias=False, activation='relu')(input_tensor)\n",
    "    # 2. Rescale\n",
    "    x = layers.Dense(input_dim, use_bias=False, activation='sigmoid')(x)\n",
    "    \n",
    "    # Scale: Reweight input features\n",
    "    return layers.Multiply()([input_tensor, x])\n",
    "\n",
    "def designed_DNN_model(outputNum, X_data=None):\n",
    "  # === 残差学习 (Residual Learning) ===\n",
    "  # 原理：模型只预测“实际增益”与“目标增益(target_gain)”之间的差值(Residual)。\n",
    "  # 最终输出 = target_gain + DNN预测的残差。\n",
    "  # 这样可以显著降低学习难度，因为大部分增益主要由 target_gain 决定，DNN 只需要微调。\n",
    "\n",
    "  inputs = layers.Input(shape=(num_inputFeatures,))\n",
    "  \n",
    "  # 1. 提取 target_gain\n",
    "  # 假设输入特征的第0列是 target_gain (根据 pd.read_csv(...).iloc[:, 3:] 的顺序)\n",
    "  target_gain = layers.Lambda(lambda x: tf.expand_dims(x[:, 0], -1), name='extract_target_gain')(inputs)\n",
    "  \n",
    "  x = inputs\n",
    "\n",
    "  # 2. 标准化 (可选，建议加上)\n",
    "  if X_data is not None:\n",
    "      normalizer = layers.Normalization(axis=-1)\n",
    "      normalizer.adapt(X_data)\n",
    "      x = normalizer(inputs)\n",
    "\n",
    "  # === SE-Block 1: Initial Feature Reweighting ===\n",
    "  # 让模型在进入深层之前，先根据全局信息自动调整输入特征的权重\n",
    "  x = se_block(x, ratio=8) \n",
    "\n",
    "  # 3. DNN 主体 (预测残差)\n",
    "  # 结构建议：Dense -> BN -> ReLU -> Dropout\n",
    "  \n",
    "  # 第一层 (保留原始)\n",
    "  x = layers.Dense(num_inputFeatures, activation='silu')(x)\n",
    "  \n",
    "  # 第二层\n",
    "  x = layers.Dense(256)(x)\n",
    "#   x = layers.BatchNormalization()(x)\n",
    "  x = layers.Activation('silu')(x)\n",
    "  x = layers.Dropout(0.1)(x) # 增加 Dropout, 比率设为 0.2\n",
    "  \n",
    "  # === SE-Block 2: Intermediate Feature Refinement ===\n",
    "  x = se_block(x, ratio=16)\n",
    "\n",
    "  # 第三层\n",
    "  x = layers.Dense(128)(x)\n",
    "#   x = layers.BatchNormalization()(x)\n",
    "  x = layers.Activation('silu')(x)\n",
    "  x = layers.Dropout(0.1)(x) # 增加 Dropout\n",
    "\n",
    "  # 第四层\n",
    "  x = layers.Dense(128)(x)\n",
    "#   x = layers.BatchNormalization()(x)\n",
    "  x = layers.Activation('silu')(x)\n",
    "  x = layers.Dropout(0.1)(x) # 增加 Dropout\n",
    "\n",
    "  # 第五层\n",
    "  x = layers.Dense(128)(x)\n",
    "#   x = layers.BatchNormalization()(x)\n",
    "  x = layers.Activation('silu')(x)\n",
    "  x = layers.Dropout(0.05)(x) # 最后一层 Dropout 稍微小点\n",
    "\n",
    "  \n",
    "  residual = layers.Dense(outputNum, name='predicted_residual')(x)\n",
    "  \n",
    "  # 4. 残差连接\n",
    "  outputs = layers.Add(name='add_residual_to_target')([residual, target_gain])\n",
    "  \n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "  model.compile(loss=custom_loss_L2, # custom_loss\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "  return model\n",
    "\n",
    "### debug function after train -> go to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db7e5acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# import gc\n",
    "\n",
    "# # 1. 读取原始数据 (加个 _raw 后缀以示区分)\n",
    "# X_train_raw = pd.read_csv(TRAIN_FEATURE_PATH).iloc[:, 3:]\n",
    "# y_train = pd.read_csv(TRAIN_LABEL_PATH)\n",
    "\n",
    "# # 2. === 应用 SHB 特征工程 ===\n",
    "# print(f\"Original features: {X_train_raw.shape[1]}\")\n",
    "# print(\"Applying SHB features to training data...\")\n",
    "# X_train = add_shb_features(X_train_raw)\n",
    "\n",
    "# # 3. === 关键：更新输入特征数量 ===\n",
    "# # 覆盖掉之前硬编码的 (Numchannels * 2 + 4)\n",
    "# global num_inputFeatures # 声明为全局变量以确保 designed_DNN_model 能读到正确的值\n",
    "# num_inputFeatures = X_train.shape[1]\n",
    "# print(f\"Features updated. New input shape: {num_inputFeatures}\")\n",
    "\n",
    "# y_train.fillna(0, inplace=True)\n",
    "\n",
    "# # K-Fold configuration\n",
    "# n_splits = 5\n",
    "# kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# # Track the best model across all folds\n",
    "# best_global_val_loss = float('inf')\n",
    "# best_global_model_path = \"\"\n",
    "\n",
    "# # K-Fold Training Loop\n",
    "# fold_no = 1\n",
    "# for train_index, val_index in kf.split(X_train):\n",
    "#     # if fold_no != 5:\n",
    "#     #     fold_no += 1\n",
    "#     #     continue\n",
    "#     print(f\"Start Training Fold {fold_no}...\")\n",
    "    \n",
    "#     # Initialize SwanLab for this fold\n",
    "#     swanlab.init(\n",
    "#         project=\"OFC_Challenge_SE_Block_WithFE\",\n",
    "#         experiment_name=f\"SILUSEFold-{fold_no}\",\n",
    "#         config={\n",
    "#             \"fold\": fold_no,\n",
    "#             \"learning_rate\": \"CosineAnnealing(Start=0.001)\",\n",
    "#             \"architecture\": \"DNN+Residual+SE-Block\",\n",
    "#             \"feature_engineering\": \"SHB_Features\",\n",
    "#             \"epochs\": 300,\n",
    "#             \"batch_size\": 64, # Default\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     # 1. 内存清理\n",
    "#     tf.keras.backend.clear_session()\n",
    "#     gc.collect()\n",
    "\n",
    "#     # 2. 数据切分\n",
    "#     X_tr, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "#     y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "#     # 3. 初始化模型 (DNN)\n",
    "#     base_model = designed_DNN_model(Numchannels, X_tr)\n",
    "    \n",
    "#     # === Cosine Annealing Learning Rate Schedule ===\n",
    "#     epochs = 300\n",
    "#     batch_size = 64\n",
    "#     if len(X_tr) > 0:\n",
    "#         steps_per_epoch = len(X_tr) // batch_size\n",
    "#     else:\n",
    "#         steps_per_epoch = 1 # Fallback safety\n",
    "        \n",
    "#     decay_steps = steps_per_epoch * epochs\n",
    "    \n",
    "#     lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "#         initial_learning_rate=0.001,\n",
    "#         decay_steps=decay_steps,\n",
    "#         alpha=0.001  # Minimum LR = 0.001 * 0.001 = 1e-6\n",
    "#     )\n",
    "    \n",
    "#     # 【修正 1】必须在这里编译！\n",
    "#     base_model.compile(\n",
    "#         loss=custom_loss_L2, \n",
    "#         optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "#     )\n",
    "\n",
    "#     # 4. 回调函数\n",
    "#     fold_model_path = f\"{model_prepath}/ML_example_model_fold{fold_no}.h5\"\n",
    "    \n",
    "#     checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "#         filepath=fold_model_path,\n",
    "#         monitor='val_loss',\n",
    "#         save_best_only=True, # 只保存在这一折里表现最好的\n",
    "#         mode='min',\n",
    "#         save_weights_only=False,\n",
    "#         verbose=0 # 设为 0 减少刷屏\n",
    "#     )\n",
    "\n",
    "#     early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "#         monitor='val_loss',\n",
    "#         patience=50,\n",
    "#         restore_best_weights=True,\n",
    "#         verbose=1\n",
    "#     )\n",
    "#         # 强烈建议加上这个，解决 Fold 2 震荡问题\n",
    "# #     lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "# #             monitor='val_loss',\n",
    "# #             factor=0.5,\n",
    "# #             patience=20,\n",
    "# #             min_lr=1e-6,\n",
    "# #             verbose=1\n",
    "# #   )  \n",
    "\n",
    "#     # 5. 训练 DNN\n",
    "#     print(f\"   [DNN] Training with SE-Block + FE (CosineAnnealing)...\")\n",
    "#     history_dnn = base_model.fit(\n",
    "#         X_tr, y_tr,\n",
    "#         validation_data=(X_val, y_val),\n",
    "#         verbose=2, \n",
    "#         epochs=epochs,\n",
    "#         batch_size=batch_size,\n",
    "#         callbacks=[checkpoint, early_stopping, SwanLabLogger()] # Add SwanLabLogger\n",
    "#     )\n",
    "#     print(f\"   [DNN] Fold {fold_no} finished. Best model saved to {fold_model_path}\")\n",
    "    \n",
    "#     fold_no += 1\n",
    "\n",
    "# print(\"K-Fold Cross Validation Completed. All models are saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1995e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def plot_loss(indx,history,ingnoreIndex):\n",
    "#     plt.figure(indx)\n",
    "#     plt.plot(history.history['loss'][ingnoreIndex:], label='loss')\n",
    "#     plt.plot(history.history['val_loss'][ingnoreIndex:], label='val_loss')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Error [gain]')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "# plot_loss(1, history_dnn, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3063f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c547f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 开始生成伪标签 (Phase 1)...\n",
      "Applying SHB features to test data...\n",
      "  -> Performing Noise Clipping (< -55dBm to -60dBm)...\n",
      "Test features shape: (21056, 216)\n",
      "Loading training labels...\n"
     ]
    }
   ],
   "source": [
    "# === 生成伪标签并合并训练集 ===\n",
    "print(\">>> 开始生成伪标签 (Phase 1)...\")\n",
    "\n",
    "# 0. 加载测试数据 (与预测函数一致)\n",
    "if not os.path.exists(TEST_FEATURE_PATH):\n",
    "    print(f\"Error: Test feature path not found: {TEST_FEATURE_PATH}\")\n",
    "else:\n",
    "    X_test_full = pd.read_csv(TEST_FEATURE_PATH)\n",
    "    X_test_raw = X_test_full.iloc[:, 5:]  # 保持切片一致\n",
    "    print(\"Applying SHB features to test data...\")\n",
    "    X_test = add_shb_features(X_test_raw)\n",
    "    print(f\"Test features shape: {X_test.shape}\")\n",
    "\n",
    "# 0.5. 确保训练数据已加载 (如果未定义)\n",
    "try:\n",
    "    y_train\n",
    "except NameError:\n",
    "    print(\"Loading training labels...\")\n",
    "    y_train = pd.read_csv(TRAIN_LABEL_PATH)\n",
    "    y_train.fillna(0, inplace=True)\n",
    "    print(f\"Training labels shape: {y_train.shape}\")\n",
    "\n",
    "try:\n",
    "    X_train\n",
    "except NameError:\n",
    "    print(\"Loading and processing training features...\")\n",
    "    X_train_raw = pd.read_csv(TRAIN_FEATURE_PATH).iloc[:, 3:]\n",
    "    print(f\"Original features: {X_train_raw.shape[1]}\")\n",
    "    print(\"Applying SHB features to training data...\")\n",
    "    X_train = add_shb_features(X_train_raw)\n",
    "    # 更新输入特征数量\n",
    "    global num_inputFeatures\n",
    "    num_inputFeatures = X_train.shape[1]\n",
    "    print(f\"Features updated. New input shape: {num_inputFeatures}\")\n",
    "\n",
    "# 1. 使用 5-Fold 模型预测测试集\n",
    "n_folds = 4\n",
    "pseudo_preds_list = []\n",
    "\n",
    "for fold in range(1, n_folds + 1):\n",
    "    fold_model_path = f\"../model/sev_se_block_fe/ML_example_model_fold{fold}.h5\"\n",
    "    if os.path.exists(fold_model_path):\n",
    "        model = tf.keras.models.load_model(fold_model_path, compile=False)\n",
    "        pred = model.predict(X_test, batch_size=256, verbose=0)\n",
    "        pseudo_preds_list.append(pred)\n",
    "        print(f\"Fold {fold} prediction done.\")\n",
    "    else:\n",
    "        print(f\"Warning: Model {fold_model_path} not found.\")\n",
    "\n",
    "if not pseudo_preds_list:\n",
    "    print(\"Error: No models found for pseudo labeling.\")\n",
    "else:\n",
    "    # 平均预测\n",
    "    pseudo_preds = np.mean(pseudo_preds_list, axis=0)\n",
    "    print(f\"Ensemble prediction shape: {pseudo_preds.shape}\")\n",
    "\n",
    "    # 2. 整理数据格式\n",
    "    pseudo_labels_df = pd.DataFrame(pseudo_preds, columns=y_train.columns)\n",
    "\n",
    "    # 3. 过滤低置信度样本 (可选，这里简单保留所有)\n",
    "    X_test_pseudo = X_test.copy()\n",
    "    y_test_pseudo = pseudo_labels_df\n",
    "\n",
    "    print(f\"生成的伪标签样本数: {len(X_test_pseudo)}\")\n",
    "\n",
    "    print(\">>> 合并训练集与伪标签数据 (Phase 2)...\")\n",
    "\n",
    "    # 1. 拼接\n",
    "    X_combined = pd.concat([X_train, X_test_pseudo], axis=0).reset_index(drop=True)\n",
    "    y_combined = pd.concat([y_train, y_test_pseudo], axis=0).reset_index(drop=True)\n",
    "\n",
    "    print(f\"原始训练集大小: {len(X_train)}\")\n",
    "    print(f\"合并后训练集大小: {len(X_combined)}\")\n",
    "\n",
    "    # 2. 重新打乱数据\n",
    "    from sklearn.utils import shuffle\n",
    "    X_combined, y_combined = shuffle(X_combined, y_combined, random_state=42)\n",
    "\n",
    "    print(\">>> 伪标签生成和合并完成！\")\n",
    "    print(f\"最终训练集大小: {len(X_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cd3ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using combined dataset: 160170 samples, 216 features\n",
      "Start Training Fold 1 with Pseudo-Labels...\n",
      "   [DNN] Training with SE-Block + FE + Pseudo-Labels (CosineAnnealing)...\n",
      "Epoch 1/300\n",
      "1113/1113 - 6s - loss: 0.2491 - val_loss: 0.2272 - 6s/epoch - 5ms/step\n",
      "Epoch 2/300\n",
      "1113/1113 - 5s - loss: 0.2217 - val_loss: 0.2094 - 5s/epoch - 4ms/step\n",
      "Epoch 3/300\n",
      "1113/1113 - 5s - loss: 0.2057 - val_loss: 0.1929 - 5s/epoch - 5ms/step\n",
      "Epoch 4/300\n",
      "1113/1113 - 5s - loss: 0.1810 - val_loss: 0.1702 - 5s/epoch - 5ms/step\n",
      "Epoch 5/300\n",
      "1113/1113 - 5s - loss: 0.1639 - val_loss: 0.1538 - 5s/epoch - 5ms/step\n",
      "Epoch 6/300\n",
      "1113/1113 - 5s - loss: 0.1539 - val_loss: 0.1599 - 5s/epoch - 4ms/step\n",
      "Epoch 7/300\n",
      "1113/1113 - 5s - loss: 0.1422 - val_loss: 0.1248 - 5s/epoch - 4ms/step\n",
      "Epoch 8/300\n",
      "1113/1113 - 5s - loss: 0.1329 - val_loss: 0.1207 - 5s/epoch - 4ms/step\n",
      "Epoch 9/300\n",
      "1113/1113 - 5s - loss: 0.1252 - val_loss: 0.1192 - 5s/epoch - 5ms/step\n",
      "Epoch 10/300\n",
      "1113/1113 - 4s - loss: 0.1250 - val_loss: 0.1158 - 4s/epoch - 4ms/step\n",
      "Epoch 11/300\n",
      "1113/1113 - 5s - loss: 0.1221 - val_loss: 0.1066 - 5s/epoch - 4ms/step\n",
      "Epoch 12/300\n",
      "1113/1113 - 5s - loss: 0.1162 - val_loss: 0.1094 - 5s/epoch - 4ms/step\n",
      "Epoch 13/300\n",
      "1113/1113 - 5s - loss: 0.1179 - val_loss: 0.1068 - 5s/epoch - 4ms/step\n",
      "Epoch 14/300\n",
      "1113/1113 - 5s - loss: 0.1142 - val_loss: 0.1041 - 5s/epoch - 4ms/step\n",
      "Epoch 15/300\n",
      "1113/1113 - 5s - loss: 0.1143 - val_loss: 0.1024 - 5s/epoch - 5ms/step\n",
      "Epoch 16/300\n",
      "1113/1113 - 4s - loss: 0.1109 - val_loss: 0.0980 - 4s/epoch - 4ms/step\n",
      "Epoch 17/300\n",
      "1113/1113 - 5s - loss: 0.1096 - val_loss: 0.1217 - 5s/epoch - 4ms/step\n",
      "Epoch 18/300\n",
      "1113/1113 - 5s - loss: 0.1077 - val_loss: 0.0966 - 5s/epoch - 4ms/step\n",
      "Epoch 19/300\n",
      "1113/1113 - 5s - loss: 0.1075 - val_loss: 0.1013 - 5s/epoch - 4ms/step\n",
      "Epoch 20/300\n",
      "1113/1113 - 5s - loss: 0.1066 - val_loss: 0.0985 - 5s/epoch - 4ms/step\n",
      "Epoch 21/300\n",
      "1113/1113 - 5s - loss: 0.1054 - val_loss: 0.0982 - 5s/epoch - 4ms/step\n",
      "Epoch 22/300\n",
      "1113/1113 - 5s - loss: 0.1037 - val_loss: 0.0970 - 5s/epoch - 4ms/step\n",
      "Epoch 23/300\n",
      "1113/1113 - 5s - loss: 0.1028 - val_loss: 0.0925 - 5s/epoch - 5ms/step\n",
      "Epoch 24/300\n",
      "1113/1113 - 5s - loss: 0.1028 - val_loss: 0.1002 - 5s/epoch - 4ms/step\n",
      "Epoch 25/300\n",
      "1113/1113 - 5s - loss: 0.0999 - val_loss: 0.0944 - 5s/epoch - 4ms/step\n",
      "Epoch 26/300\n",
      "1113/1113 - 5s - loss: 0.0994 - val_loss: 0.0987 - 5s/epoch - 4ms/step\n",
      "Epoch 27/300\n",
      "1113/1113 - 5s - loss: 0.0988 - val_loss: 0.0936 - 5s/epoch - 4ms/step\n",
      "Epoch 28/300\n",
      "1113/1113 - 4s - loss: 0.0983 - val_loss: 0.0956 - 4s/epoch - 4ms/step\n",
      "Epoch 29/300\n",
      "1113/1113 - 5s - loss: 0.0997 - val_loss: 0.0865 - 5s/epoch - 5ms/step\n",
      "Epoch 30/300\n",
      "1113/1113 - 4s - loss: 0.1016 - val_loss: 0.0855 - 4s/epoch - 4ms/step\n",
      "Epoch 31/300\n",
      "1113/1113 - 4s - loss: 0.0984 - val_loss: 0.0876 - 4s/epoch - 4ms/step\n",
      "Epoch 32/300\n",
      "1113/1113 - 5s - loss: 0.0989 - val_loss: 0.0876 - 5s/epoch - 4ms/step\n",
      "Epoch 33/300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-52e806df8630>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# 5. 训练 DNN (使用伪标签数据)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   [DNN] Training with SE-Block + FE + Pseudo-Labels (CosineAnnealing)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     history_dnn = base_model.fit(\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1418\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1420\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1421\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1717\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m-> 2955\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3243\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3244\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize_function_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3245\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3246\u001b[0m       \u001b[0mflat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcanonicalize_function_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2765\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2766\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_numpy_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2767\u001b[0;31m       \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_numpy_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2768\u001b[0m       \u001b[0mflat_inputs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mflat_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2769\u001b[0m       \u001b[0mfiltered_flat_inputs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfiltered_flat_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# === 使用合并数据进行第二次训练 (Pseudo-Labeling Training) ===\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# SnapshotCallback 定义\n",
    "class SnapshotCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, model_dir, n_cycles=5, total_epochs=300):\n",
    "        self.model_dir = model_dir\n",
    "        self.cycle_len = total_epochs // n_cycles\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.cycle_len == 0:\n",
    "            filename = f\"{self.model_dir}/snapshot_model_epoch_{epoch+1}.h5\"\n",
    "            self.model.save(filename)\n",
    "            print(f\"Snapshot saved: {filename}\")\n",
    "\n",
    "# 使用合并后的数据进行训练\n",
    "print(f\"Using combined dataset: {X_combined.shape[0]} samples, {X_combined.shape[1]} features\")\n",
    "\n",
    "# K-Fold configuration\n",
    "n_splits = 9\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Track the best model across all folds\n",
    "best_global_val_loss = float('inf')\n",
    "best_global_model_path = \"\"\n",
    "\n",
    "# K-Fold Training Loop with Pseudo-Labels\n",
    "fold_no = 1\n",
    "\n",
    "for train_index, val_index in kf.split(X_combined):\n",
    "    print(f\"Start Training Fold {fold_no} with Pseudo-Labels...\")\n",
    "\n",
    "    # 1. 内存清理\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    # 2. 数据切分 (使用合并数据)\n",
    "    X_tr, X_val = X_combined.iloc[train_index], X_combined.iloc[val_index]\n",
    "    y_tr, y_val = y_combined.iloc[train_index], y_combined.iloc[val_index]\n",
    "\n",
    "    # 3. 初始化模型 (DNN)\n",
    "    base_model = designed_DNN_model(Numchannels, X_tr)\n",
    "\n",
    "    # === Cosine Annealing Learning Rate Schedule ===\n",
    "    epochs = 300\n",
    "    batch_size = 128\n",
    "    if len(X_tr) > 0:\n",
    "        steps_per_epoch = len(X_tr) // batch_size\n",
    "    else:\n",
    "        steps_per_epoch = 1\n",
    "\n",
    "    decay_steps = steps_per_epoch * epochs\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=0.001,\n",
    "        decay_steps=decay_steps,\n",
    "        alpha=0.001\n",
    ")\n",
    "\n",
    "    base_model.compile(\n",
    "        loss=custom_loss_L2,\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    ")\n",
    "\n",
    "    # 4. 回调函数\n",
    "    fold_model_path = f\"{model_prepath}/ML_example_model_pseudo_fold{fold_no}.h5\"\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=fold_model_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        save_weights_only=False,\n",
    "        verbose=0\n",
    ")\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    ")\n",
    "\n",
    "    # === 新增：SnapshotCallback ===\n",
    "    snapshot_cb = SnapshotCallback(model_prepath, n_cycles=5, total_epochs=300)\n",
    "\n",
    "    # 5. 训练 DNN (使用伪标签数据)\n",
    "    print(f\"   [DNN] Training with SE-Block + FE + Pseudo-Labels (CosineAnnealing)...\")\n",
    "    history_dnn = base_model.fit(\n",
    "        X_tr, y_tr,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=2,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[checkpoint, early_stopping, snapshot_cb]\n",
    ")\n",
    "    print(f\"   [DNN] Fold {fold_no} finished. Best model saved to {fold_model_path}\")\n",
    "\n",
    "    fold_no += 1\n",
    "\n",
    "print(\"Pseudo-Labeling K-Fold Cross Validation Completed. All models are saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3186559f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> FUNCTION START: run_ensemble_prediction\n",
      "Applying SHB features to test data...\n",
      "  -> Performing Noise Clipping (< -55dBm to -60dBm)...\n",
      "Test features shape: (21056, 216)\n",
      "  -> Performing Noise Clipping (< -55dBm to -60dBm)...\n",
      "--- Starting loop for 4 folds from ../model/sev_se_block_fe/ ---\n",
      "   Processing Fold 1/4 ...\n",
      "   Processing Fold 2/4 ...\n",
      "   Processing Fold 3/4 ...\n",
      "   Processing Fold 4/4 ...\n",
      "Found 1090 unique WSS patterns in Test Set.\n",
      "Corrected 0 samples using WSS-Aware KNN.\n",
      "SUCCESS: Submission saved to ../Features/Test/submission_se_block_fe_snapshot_main2.csv\n",
      ">>> FUNCTION END\n"
     ]
    }
   ],
   "source": [
    "# === WRAPPED PREDICTION FUNCTION ===\n",
    "def predict_with_tta(model, X_test, n_iter=10, noise_std=0.01):\n",
    "    preds = []\n",
    "    preds.append(model.predict(X_test))\n",
    "    for i in range(n_iter):\n",
    "        X_test_aug = X_test.copy()\n",
    "        noise = np.random.normal(0, noise_std, X_test_aug.iloc[:, :95].shape)\n",
    "        X_test_aug.iloc[:, :95] += noise\n",
    "        preds.append(model.predict(X_test_aug))\n",
    "    return np.mean(preds, axis=0)\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def apply_wss_aware_knn(X_train, y_train, X_test, y_pred_test, alpha=0.3):\n",
    "    wss_cols = [c for c in X_train.columns if 'DUT_WSS_activated_channel_index' in c]\n",
    "    wss_cols.sort()\n",
    "    def get_pattern_id(df):\n",
    "        return df[wss_cols].astype(str).agg(''.join, axis=1)\n",
    "    train_patterns = get_pattern_id(X_train)\n",
    "    test_patterns = get_pattern_id(X_test)\n",
    "    y_final = y_pred_test.copy()\n",
    "    unique_patterns = test_patterns.unique()\n",
    "    print(f\"Found {len(unique_patterns)} unique WSS patterns in Test Set.\")\n",
    "    corrections_count = 0\n",
    "    for pat in unique_patterns:\n",
    "        train_indices = train_patterns[train_patterns == pat].index\n",
    "        if len(train_indices) < 5:\n",
    "            continue\n",
    "        spectra_cols = [c for c in X_train.columns if 'EDFA_input_spectra' in c]\n",
    "        X_train_sub = X_train.loc[train_indices, spectra_cols].values\n",
    "        y_train_sub = y_train.loc[train_indices].values\n",
    "        test_indices = test_patterns[test_patterns == pat].index\n",
    "        X_test_sub = X_test.loc[test_indices, spectra_cols].values\n",
    "        knn = NearestNeighbors(n_neighbors=min(10, len(X_train_sub)), metric='euclidean')\n",
    "        knn.fit(X_train_sub)\n",
    "        distances, neighbor_idx = knn.kneighbors(X_test_sub)\n",
    "        for i, idx_in_test in enumerate(test_indices):\n",
    "            neighbor_means = np.mean(y_train_sub[neighbor_idx[i]], axis=0)\n",
    "            if distances[i][0] < 2.0:\n",
    "                y_final[idx_in_test] = (1 - alpha) * y_final[idx_in_test] + alpha * neighbor_means\n",
    "                corrections_count += 1\n",
    "    print(f\"Corrected {corrections_count} samples using WSS-Aware KNN.\")\n",
    "    return y_final\n",
    "\n",
    "def run_ensemble_prediction():\n",
    "    print(\">>> FUNCTION START: run_ensemble_prediction\")\n",
    "    if not os.path.exists(TEST_FEATURE_PATH):\n",
    "        print(f\"Error: Test feature path not found: {TEST_FEATURE_PATH}\")\n",
    "        return\n",
    "    X_test_full = pd.read_csv(TEST_FEATURE_PATH)\n",
    "    X_test_raw = X_test_full.iloc[:, 5:]\n",
    "    print(\"Applying SHB features to test data...\")\n",
    "    X_test = add_shb_features(X_test_raw)\n",
    "    print(f\"Test features shape: {X_test.shape}\")\n",
    "    n_splits = 4\n",
    "    y_pred_ensemble = None\n",
    "    ensemble_model_path = \"../model/sev_se_block_fe/\"\n",
    "    TRAIN_LABEL_PATH = \"../Features/Train/train_labels.csv\"\n",
    "    y_train = pd.read_csv(TRAIN_LABEL_PATH)\n",
    "    TRAIN_FEATURE_PATH = \"../Features/Train/train_features.csv\"\n",
    "    X_train_raw = pd.read_csv(TRAIN_FEATURE_PATH).iloc[:, 3:]\n",
    "    X_train = add_shb_features(X_train_raw)\n",
    "    print(f\"--- Starting loop for {n_splits} folds from {ensemble_model_path} ---\")\n",
    "    for i in range(1, n_splits + 1):\n",
    "        print(f\"   Processing Fold {i}/{n_splits} ...\")\n",
    "        preds = []\n",
    "        # 主模型\n",
    "        fold_model_path = f\"{ensemble_model_path}/ML_example_model_fold{i}.h5\"\n",
    "        if os.path.exists(fold_model_path):\n",
    "            try:\n",
    "                model_fold = tf.keras.models.load_model(fold_model_path, compile=False)\n",
    "                pred_main = predict_with_tta(model_fold, X_test, n_iter=5, noise_std=0.01)\n",
    "                preds.append(pred_main)\n",
    "            except Exception as e:\n",
    "                print(f\"     [!] DNN Exception fold {i}: {e}\")\n",
    "        else:\n",
    "            print(f\"     [!] DNN Model file missing: {fold_model_path}\")\n",
    "        # 快照1\n",
    "        snap1_path = f\"{ensemble_model_path}/snapshot_model_epoch_60.h5\"\n",
    "        if os.path.exists(snap1_path):\n",
    "            try:\n",
    "                snap1_model = tf.keras.models.load_model(snap1_path, compile=False)\n",
    "                pred_snap1 = predict_with_tta(snap1_model, X_test, n_iter=5, noise_std=0.01)\n",
    "                preds.append(pred_snap1)\n",
    "            except Exception as e:\n",
    "                print(f\"     [!] Snapshot Exception epoch 60: {e}\")\n",
    "        else:\n",
    "            print(f\"     [!] Snapshot file missing: {snap1_path}\")\n",
    "        # 快照2\n",
    "        snap2_path = f\"{ensemble_model_path}/snapshot_model_epoch_120.h5\"\n",
    "        if os.path.exists(snap2_path):\n",
    "            try:\n",
    "                snap2_model = tf.keras.models.load_model(snap2_path, compile=False)\n",
    "                pred_snap2 = predict_with_tta(snap2_model, X_test, n_iter=5, noise_std=0.01)\n",
    "                preds.append(pred_snap2)\n",
    "            except Exception as e:\n",
    "                print(f\"     [!] Snapshot Exception epoch 120: {e}\")\n",
    "        else:\n",
    "            print(f\"     [!] Snapshot file missing: {snap2_path}\")\n",
    "        当前折的平均\n",
    "        if preds:\n",
    "            fold_pred = np.mean(preds, axis=0)\n",
    "            if y_pred_ensemble is None:\n",
    "                y_pred_ensemble = fold_pred\n",
    "            else:\n",
    "                y_pred_ensemble += fold_pred\n",
    "    if y_pred_ensemble is None:\n",
    "        print(\"Error: No predictions were made.\")\n",
    "        return\n",
    "    y_pred_dnn = y_pred_ensemble / n_splits\n",
    "    y_submission = apply_wss_aware_knn(X_train, y_train, X_test, y_pred_dnn, alpha=0.3)\n",
    "    y_pred = pd.DataFrame(y_submission, columns=y_train.columns)\n",
    "    wss_cols = [col for col in X_test.columns if 'dut_wss_activated_channel_index' in col.lower()]\n",
    "    label_cols = [col for col in y_train.columns if 'calculated_gain_spectra' in col.lower()]\n",
    "    mask = X_test[wss_cols].values == 1\n",
    "    y_pred = pd.DataFrame(np.where(mask, y_pred.values, np.nan), columns=label_cols)\n",
    "    y_pred.fillna(0, inplace=True)\n",
    "    kaggle_ID = X_test_full.columns[0]\n",
    "    y_pred.insert(0, kaggle_ID, X_test_full[kaggle_ID].values)\n",
    "    output_path = \"../Features/Test/submission_se_block_fe_snapshot_main2.csv\"\n",
    "    y_pred.to_csv(output_path, index=False)\n",
    "    print(f\"SUCCESS: Submission saved to {output_path}\")\n",
    "    print(\">>> FUNCTION END\")\n",
    "\n",
    "run_ensemble_prediction()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
